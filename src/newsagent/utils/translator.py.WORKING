# ── src/newsagent/utils/translator.py ──────────────────────────────────────
"""
translate(text, lang) -> str

• Streams Markdown in ≤1 500-word chunks; automatic split on 413 errors.
• Shared exponential back-off → practically zero 429s.
• Per-chunk and whole-doc cache in /cache/translate/.
"""

from __future__ import annotations
import hashlib, json, os, random, re, time, textwrap
from pathlib import Path
from typing import Dict, List

import requests

# ── settings ───────────────────────────────────────────────────────────────
CACHE_DIR   = Path(__file__).resolve().parents[2] / "cache" / "translate"
CACHE_DIR.mkdir(parents=True, exist_ok=True)

GROQ_ENDPOINT = "https://api.groq.com/openai/v1/chat/completions"
MODEL         = "llama3-8b-8192"
CHUNK_WORDS   = 1_500                       # ≈ 6 k tokens – safe margin
MIN_WORDS     = 250                         # don’t split below this
BASE_DELAY    = 2.0
MAX_DELAY     = 60.0
_BOILER       = re.compile(r"^\s*translation\s*:\s*", re.I)

# ── tiny helpers ───────────────────────────────────────────────────────────
def _key(txt: str, lang: str) -> Path:
    h = hashlib.sha256(f"{lang}::{txt}".encode()).hexdigest()[:16]
    return CACHE_DIR / f"{lang}_{h}.json"

def _get_cached(txt: str, lang: str) -> str | None:
    fp = _key(txt, lang)
    if fp.exists():
        return json.loads(fp.read_text())["t"]
    return None

def _save(txt: str, lang: str, out: str) -> None:
    _key(txt, lang).write_text(json.dumps({"t": out}), encoding="utf-8")

def _post(payload: Dict, key: str) -> requests.Response:
    hdrs = {"Authorization": f"Bearer {key}",
            "Content-Type": "application/json"}
    return requests.post(GROQ_ENDPOINT, headers=hdrs,
                         json=payload, timeout=90)

def _iso(lang_tag: str) -> str:
    """Map our ENG/DEU/SVE/POL → Groq ISO-639-1 codes."""
    return {
        "ENG": "en",  "DEU": "de",
        "SVE": "sv",  "POL": "pl",
    }.get(lang_tag.upper(), lang_tag.lower())

# ── chunk translator ───────────────────────────────────────────────────────
def _translate_chunk(chunk: str, lang_iso: str, api_key: str,
                     delay: float) -> str:
    prompt = textwrap.dedent(f"""\
        You are a professional translator. Convert the following GitHub-
        flavoured Markdown **verbatim** to **{lang_iso}**. Preserve headings,
        links and bullet structure exactly; translate only human-readable
        text. Do **NOT** add extra headers or footers.""")
    payload = {
        "model": MODEL,
        "messages": [
            {"role": "system", "content": prompt},
            {"role": "user",   "content": chunk},
        ],
        "max_tokens": 2048,
    }

    retries = 5
    for attempt in range(retries):
        r = _post(payload, api_key)
        if r.status_code == 429:                       # rate-limit
            delay = min(MAX_DELAY, delay * 1.7 + random.uniform(0, 2))
            print(f"    429 – sleeping {delay:.1f}s …")
            time.sleep(delay)
            continue
        if r.status_code == 413:                       # chunk still too big
            if len(chunk.split()) <= MIN_WORDS:
                raise RuntimeError("chunk too big even at MIN_WORDS")
            mid = len(chunk.split()) // 2
            left  = " ".join(chunk.split()[:mid])
            right = " ".join(chunk.split()[mid:])
            return (_translate_chunk(left,  lang_iso, api_key, delay) + "\n" +
                    _translate_chunk(right, lang_iso, api_key, delay))
        r.raise_for_status()
        out = r.json()["choices"][0]["message"]["content"]
        return _BOILER.sub("", out).lstrip()
    raise RuntimeError("too many retries")

# ── public API ─────────────────────────────────────────────────────────────
def translate(text: str, lang_tag: str) -> str:
    if lang_tag.upper() == "ENG":
        return text
    if cached := _get_cached(text, lang_tag):
        return cached

    api_key = os.getenv("GROQ_API_KEY") or ""
    if not api_key:
        raise RuntimeError("GROQ_API_KEY missing")

    # Split into coarse chunks first
    words = text.split()
    chunks = [" ".join(words[i:i + CHUNK_WORDS])
              for i in range(0, len(words), CHUNK_WORDS)]

    out_parts: List[str] = []
    delay = BASE_DELAY
    lang_iso = _iso(lang_tag)

    for idx, chunk in enumerate(chunks, 1):
        if cached := _get_cached(chunk, lang_tag):      # chunk-level cache
            out_parts.append(cached)
            continue
        print(f"    ↳ translating chunk {idx}/{len(chunks)} …")
        translated = _translate_chunk(chunk, lang_iso, api_key, delay)
        _save(chunk, lang_tag, translated)
        out_parts.append(translated)

    full = "\n".join(out_parts)
    _save(text, lang_tag, full)                         # whole-doc cache
    return full
